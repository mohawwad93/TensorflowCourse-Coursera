{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week 2 ─ Cats vs Dogs using data augmentation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"dn-6c02VmqiN","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599884522978,"user_tz":-180,"elapsed":2905,"user":{"displayName":"Mohammed Awwad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqiTeQpcM-CfF6Ucam6keXozOyGduvzfYqq6qb=s64","userId":"03390490958354444129"}}},"source":["# In this exercise you will train a CNN on the FULL Cats-v-dogs dataset\n","# This will require you doing a lot of data preprocessing because\n","# the dataset isn't split into training and validation for you\n","# This code block has all the required inputs\n","import os\n","import zipfile\n","import random\n","import tensorflow as tf\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from shutil import copyfile"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"3sd9dQWa23aj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":208},"executionInfo":{"status":"ok","timestamp":1599884544362,"user_tz":-180,"elapsed":23779,"user":{"displayName":"Mohammed Awwad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqiTeQpcM-CfF6Ucam6keXozOyGduvzfYqq6qb=s64","userId":"03390490958354444129"}},"outputId":"6330636b-fe08-4e8a-92e4-b4d15b73102b"},"source":["# This code block downloads the full Cats-v-Dogs dataset and stores it as \n","# cats-and-dogs.zip. It then unzips it to /tmp\n","# which will create a tmp/PetImages directory containing subdirectories\n","# called 'Cat' and 'Dog' (that's how the original researchers structured it)\n","# If the URL doesn't work, \n","# .   visit https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765\n","# And right click on the 'Download Manually' link to get a new URL\n","\n","!wget --no-check-certificate \\\n","    \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\" \\\n","    -O \"/tmp/cats-and-dogs.zip\"\n","\n","local_zip = '/tmp/cats-and-dogs.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('/tmp')\n","zip_ref.close()\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["--2020-09-12 04:22:03--  https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\n","Resolving download.microsoft.com (download.microsoft.com)... 23.215.52.166, 2a02:26f0:fe00:1af::e59, 2a02:26f0:fe00:1a1::e59\n","Connecting to download.microsoft.com (download.microsoft.com)|23.215.52.166|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 824894548 (787M) [application/octet-stream]\n","Saving to: ‘/tmp/cats-and-dogs.zip’\n","\n","/tmp/cats-and-dogs. 100%[===================>] 786.68M  71.3MB/s    in 9.5s    \n","\n","2020-09-12 04:22:12 (83.0 MB/s) - ‘/tmp/cats-and-dogs.zip’ saved [824894548/824894548]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gi3yD62a6X3S","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1599884544367,"user_tz":-180,"elapsed":23267,"user":{"displayName":"Mohammed Awwad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqiTeQpcM-CfF6Ucam6keXozOyGduvzfYqq6qb=s64","userId":"03390490958354444129"}},"outputId":"22f29b36-4b3e-4a5e-dc01-7b02c7fceba4"},"source":["print(len(os.listdir('/tmp/PetImages/Cat/')))\n","print(len(os.listdir('/tmp/PetImages/Dog/')))\n","\n","# Expected Output:\n","# 12501\n","# 12501"],"execution_count":4,"outputs":[{"output_type":"stream","text":["12501\n","12501\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F-QkLjxpmyK2","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599884544371,"user_tz":-180,"elapsed":22840,"user":{"displayName":"Mohammed Awwad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqiTeQpcM-CfF6Ucam6keXozOyGduvzfYqq6qb=s64","userId":"03390490958354444129"}}},"source":["# Use os.mkdir to create your directories\n","# You will need a directory for cats-v-dogs, and subdirectories for training\n","# and testing. These in turn will need subdirectories for 'cats' and 'dogs'\n","try:\n","    #YOUR CODE GOES HERE\n","    os.mkdir('/tmp/cats-v-dogs')\n","    os.mkdir('/tmp/cats-v-dogs/training')\n","    os.mkdir('/tmp/cats-v-dogs/testing')\n","    os.mkdir('/tmp/cats-v-dogs/training/cats')\n","    os.mkdir('/tmp/cats-v-dogs/training/dogs')\n","    os.mkdir('/tmp/cats-v-dogs/testing/cats')\n","    os.mkdir('/tmp/cats-v-dogs/testing/dogs')\n","except OSError:\n","    pass"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"zvSODo0f9LaU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1599884560982,"user_tz":-180,"elapsed":39164,"user":{"displayName":"Mohammed Awwad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqiTeQpcM-CfF6Ucam6keXozOyGduvzfYqq6qb=s64","userId":"03390490958354444129"}},"outputId":"43c2758b-bfa2-4f4b-cc2c-ab31c092cabc"},"source":["# Write a python function called split_data which takes\n","# a SOURCE directory containing the files\n","# a TRAINING directory that a portion of the files will be copied to\n","# a TESTING directory that a portion of the files will be copie to\n","# a SPLIT SIZE to determine the portion\n","# The files should also be randomized, so that the training set is a random\n","# X% of the files, and the test set is the remaining files\n","# SO, for example, if SOURCE is PetImages/Cat, and SPLIT SIZE is .9\n","# Then 90% of the images in PetImages/Cat will be copied to the TRAINING dir\n","# and 10% of the images will be copied to the TESTING dir\n","# Also -- All images should be checked, and if they have a zero file length,\n","# they will not be copied over\n","#\n","# os.listdir(DIRECTORY) gives you a listing of the contents of that directory\n","# os.path.getsize(PATH) gives you the size of the file\n","# copyfile(source, destination) copies a file from source to destination\n","# random.sample(list, len(list)) shuffles a list\n","def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n","# YOUR CODE STARTS HERE\n","    filenames = []\n","    for fn in os.listdir(SOURCE):\n","       if os.path.getsize(os.path.join(SOURCE, fn)) > 0:\n","         filenames.append(fn)\n","       else:\n","         print(fn + \" is zero length, so ignoring\")\n","    \n","    DATA_STORE_SIZE = len(filenames)\n","    filenames = random.sample(filenames, DATA_STORE_SIZE)\n","    TRAIN_SIZE =  int(DATA_STORE_SIZE * SPLIT_SIZE)\n","    train_set = filenames[:TRAIN_SIZE]\n","    test_set = filenames[TRAIN_SIZE:]\n","\n","    for fn in train_set:\n","      copyfile(os.path.join(SOURCE, fn), os.path.join(TRAINING, fn))\n","\n","    for fn in test_set:\n","      copyfile(os.path.join(SOURCE, fn), os.path.join(TESTING, fn))\n","\n","# YOUR CODE ENDS HERE\n","\n","\n","CAT_SOURCE_DIR = \"/tmp/PetImages/Cat/\"\n","TRAINING_CATS_DIR = \"/tmp/cats-v-dogs/training/cats/\"\n","TESTING_CATS_DIR = \"/tmp/cats-v-dogs/testing/cats/\"\n","DOG_SOURCE_DIR = \"/tmp/PetImages/Dog/\"\n","TRAINING_DOGS_DIR = \"/tmp/cats-v-dogs/training/dogs/\"\n","TESTING_DOGS_DIR = \"/tmp/cats-v-dogs/testing/dogs/\"\n","\n","split_size = .9\n","split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)\n","split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)\n","\n","# Expected output\n","# 666.jpg is zero length, so ignoring\n","# 11702.jpg is zero length, so ignoring"],"execution_count":6,"outputs":[{"output_type":"stream","text":["666.jpg is zero length, so ignoring\n","11702.jpg is zero length, so ignoring\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"luthalB76ufC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":86},"executionInfo":{"status":"ok","timestamp":1599884560985,"user_tz":-180,"elapsed":38905,"user":{"displayName":"Mohammed Awwad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqiTeQpcM-CfF6Ucam6keXozOyGduvzfYqq6qb=s64","userId":"03390490958354444129"}},"outputId":"0aa39bca-ac11-46ef-86c4-c2ae9722a0ab"},"source":["print(len(os.listdir('/tmp/cats-v-dogs/training/cats/')))\n","print(len(os.listdir('/tmp/cats-v-dogs/training/dogs/')))\n","print(len(os.listdir('/tmp/cats-v-dogs/testing/cats/')))\n","print(len(os.listdir('/tmp/cats-v-dogs/testing/dogs/')))\n","\n","# Expected output:\n","# 11250\n","# 11250\n","# 1250\n","# 1250"],"execution_count":7,"outputs":[{"output_type":"stream","text":["11250\n","11250\n","1250\n","1250\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-BQrav4anTmj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":607},"executionInfo":{"status":"ok","timestamp":1599892501632,"user_tz":-180,"elapsed":3045,"user":{"displayName":"Mohammed Awwad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqiTeQpcM-CfF6Ucam6keXozOyGduvzfYqq6qb=s64","userId":"03390490958354444129"}},"outputId":"244cdafc-fcaf-4917-a146-58f1d626bf59"},"source":["# DEFINE A KERAS MODEL TO CLASSIFY CATS V DOGS\n","# USE AT LEAST 3 CONVOLUTION LAYERS\n","model = tf.keras.models.Sequential([\n","# YOUR CODE HERE\n","  tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n","  tf.keras.layers.MaxPool2D(2, 2),  \n","  tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n","  tf.keras.layers.MaxPool2D(2, 2),\n","  tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n","  tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n","  tf.keras.layers.MaxPool2D(2, 2),\n","  tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n","  tf.keras.layers.MaxPool2D(2, 2),\n","  tf.keras.layers.Flatten(),\n","  tf.keras.layers.Dense(1024, activation='relu'),\n","  tf.keras.layers.Dense(1024, activation='relu'),\n","  tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(optimizer=RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['acc'])\n","model.summary()"],"execution_count":97,"outputs":[{"output_type":"stream","text":["Model: \"sequential_37\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_162 (Conv2D)          (None, 148, 148, 32)      896       \n","_________________________________________________________________\n","max_pooling2d_130 (MaxPoolin (None, 74, 74, 32)        0         \n","_________________________________________________________________\n","conv2d_163 (Conv2D)          (None, 72, 72, 64)        18496     \n","_________________________________________________________________\n","max_pooling2d_131 (MaxPoolin (None, 36, 36, 64)        0         \n","_________________________________________________________________\n","conv2d_164 (Conv2D)          (None, 34, 34, 64)        36928     \n","_________________________________________________________________\n","conv2d_165 (Conv2D)          (None, 32, 32, 64)        36928     \n","_________________________________________________________________\n","max_pooling2d_132 (MaxPoolin (None, 16, 16, 64)        0         \n","_________________________________________________________________\n","conv2d_166 (Conv2D)          (None, 14, 14, 128)       73856     \n","_________________________________________________________________\n","max_pooling2d_133 (MaxPoolin (None, 7, 7, 128)         0         \n","_________________________________________________________________\n","flatten_37 (Flatten)         (None, 6272)              0         \n","_________________________________________________________________\n","dense_92 (Dense)             (None, 1024)              6423552   \n","_________________________________________________________________\n","dense_93 (Dense)             (None, 1024)              1049600   \n","_________________________________________________________________\n","dense_94 (Dense)             (None, 1)                 1025      \n","=================================================================\n","Total params: 7,641,281\n","Trainable params: 7,641,281\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mlNjoJ5D61N6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1599892503750,"user_tz":-180,"elapsed":1064,"user":{"displayName":"Mohammed Awwad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqiTeQpcM-CfF6Ucam6keXozOyGduvzfYqq6qb=s64","userId":"03390490958354444129"}},"outputId":"aa439835-ed51-447f-cd3c-f6a728a9498f"},"source":["TRAINING_DIR = \"/tmp/cats-v-dogs/training\" #YOUR CODE HERE\n","train_datagen = ImageDataGenerator(rescale=1./255,\n","      rotation_range=40,\n","      width_shift_range=0.2,\n","      height_shift_range=0.2,\n","      shear_range=0.2,\n","      zoom_range=0.2,\n","      horizontal_flip=True,\n","      fill_mode='nearest') #YOUR CODE HERE\n","train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n","                                                    target_size=(150, 150),\n","                                                    batch_size=128,\n","                                                    class_mode='binary') #YOUR CODE HERE\n","\n","VALIDATION_DIR = \"/tmp/cats-v-dogs/testing\"#YOUR CODE HERE\n","\n","validation_datagen = ImageDataGenerator(rescale=1./255) #YOUR CODE HERE\n","validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n","                                                    target_size=(150, 150),\n","                                                    batch_size=32,\n","                                                    class_mode='binary') #YOUR CODE HERE\n","\n","\n","# Expected Output:\n","# Found 22498 images belonging to 2 classes.\n","# Found 2500 images belonging to 2 classes."],"execution_count":98,"outputs":[{"output_type":"stream","text":["Found 22499 images belonging to 2 classes.\n","Found 2499 images belonging to 2 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KyS4n53w7DxC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":801},"outputId":"f471c6e6-c06d-4daa-81b8-c2c3c96ce099"},"source":["history = model.fit_generator(train_generator,\n","                              epochs=15,\n","                              verbose=1,\n","                              validation_data=validation_generator)\n","\n","# The expectation here is that the model will train, and that accuracy will be > 95% on both training and validation\n","# i.e. acc:A1 and val_acc:A2 will be visible, and both A1 and A2 will be > .9"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/15\n","106/176 [=================>............] - ETA: 1:04 - loss: 0.6931 - acc: 0.5256"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 32 bytes but only got 0. Skipping tag 270\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 5 bytes but only got 0. Skipping tag 271\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 272\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 282\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 283\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 20 bytes but only got 0. Skipping tag 306\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:770: UserWarning: Possibly corrupt EXIF data.  Expecting to read 48 bytes but only got 0. Skipping tag 532\n","  \" Skipping tag %s\" % (size, len(data), tag)\n","/usr/local/lib/python3.6/dist-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n","  warnings.warn(str(msg))\n"],"name":"stderr"},{"output_type":"stream","text":["176/176 [==============================] - 173s 981ms/step - loss: 0.6898 - acc: 0.5369 - val_loss: 0.6752 - val_acc: 0.5958\n","Epoch 2/15\n","176/176 [==============================] - 171s 970ms/step - loss: 0.6784 - acc: 0.5917 - val_loss: 1.4168 - val_acc: 0.5066\n","Epoch 3/15\n","176/176 [==============================] - 171s 973ms/step - loss: 0.6376 - acc: 0.6424 - val_loss: 0.5843 - val_acc: 0.7047\n","Epoch 4/15\n","176/176 [==============================] - 172s 975ms/step - loss: 0.6058 - acc: 0.6777 - val_loss: 0.5443 - val_acc: 0.7403\n","Epoch 5/15\n","176/176 [==============================] - 171s 970ms/step - loss: 0.5754 - acc: 0.7051 - val_loss: 0.5136 - val_acc: 0.7627\n","Epoch 6/15\n","176/176 [==============================] - 172s 978ms/step - loss: 0.5524 - acc: 0.7238 - val_loss: 0.4484 - val_acc: 0.7899\n","Epoch 7/15\n","176/176 [==============================] - 173s 981ms/step - loss: 0.5291 - acc: 0.7390 - val_loss: 0.4904 - val_acc: 0.7755\n","Epoch 8/15\n","176/176 [==============================] - 172s 974ms/step - loss: 0.5138 - acc: 0.7506 - val_loss: 0.6371 - val_acc: 0.6635\n","Epoch 9/15\n","176/176 [==============================] - 171s 971ms/step - loss: 0.4937 - acc: 0.7638 - val_loss: 0.6105 - val_acc: 0.7407\n","Epoch 10/15\n","176/176 [==============================] - 170s 968ms/step - loss: 0.4706 - acc: 0.7798 - val_loss: 0.4648 - val_acc: 0.7663\n","Epoch 11/15\n","176/176 [==============================] - 171s 972ms/step - loss: 0.4430 - acc: 0.7983 - val_loss: 0.3315 - val_acc: 0.8519\n","Epoch 12/15\n","176/176 [==============================] - 172s 977ms/step - loss: 0.4199 - acc: 0.8074 - val_loss: 0.3532 - val_acc: 0.8459\n","Epoch 13/15\n","176/176 [==============================] - 171s 972ms/step - loss: 0.4047 - acc: 0.8185 - val_loss: 0.3172 - val_acc: 0.8531\n","Epoch 14/15\n"," 73/176 [===========>..................] - ETA: 1:34 - loss: 0.3843 - acc: 0.8310"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MWZrJN4-65RC","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1599850139872,"user_tz":-180,"elapsed":164811,"user":{"displayName":"Mohammed Awwad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqiTeQpcM-CfF6Ucam6keXozOyGduvzfYqq6qb=s64","userId":"03390490958354444129"}}},"source":["# PLOT LOSS AND ACCURACY\n","%matplotlib inline\n","\n","import matplotlib.image  as mpimg\n","import matplotlib.pyplot as plt\n","\n","#-----------------------------------------------------------\n","# Retrieve a list of list results on training and test data\n","# sets for each training epoch\n","#-----------------------------------------------------------\n","acc=history.history['acc']\n","val_acc=history.history['val_acc']\n","loss=history.history['loss']\n","val_loss=history.history['val_loss']\n","\n","epochs=range(len(acc)) # Get number of epochs\n","\n","#------------------------------------------------\n","# Plot training and validation accuracy per epoch\n","#------------------------------------------------\n","plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n","plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n","plt.title('Training and validation accuracy')\n","plt.figure()\n","\n","#------------------------------------------------\n","# Plot training and validation loss per epoch\n","#------------------------------------------------\n","plt.plot(epochs, loss, 'r', \"Training Loss\")\n","plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n","\n","\n","plt.title('Training and validation loss')\n","\n","# Desired output. Charts with training and validation metrics. No crash :)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LqL6FYUrtXpf","colab_type":"code","colab":{},"executionInfo":{"status":"aborted","timestamp":1599850139877,"user_tz":-180,"elapsed":164471,"user":{"displayName":"Mohammed Awwad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjqiTeQpcM-CfF6Ucam6keXozOyGduvzfYqq6qb=s64","userId":"03390490958354444129"}}},"source":["# Here's a codeblock just for fun. You should be able to upload an image here \n","# and have it classified without crashing\n","\n","import numpy as np\n","from google.colab import files\n","from keras.preprocessing import image\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n"," \n","  # predicting images\n","  path = '/content/' + fn\n","  img = image.load_img(path, target_size=(# YOUR CODE HERE))\n","  x = image.img_to_array(img)\n","  x = np.expand_dims(x, axis=0)\n","\n","  images = np.vstack([x])\n","  classes = model.predict(images, batch_size=10)\n","  print(classes[0])\n","  if classes[0]>0.5:\n","    print(fn + \" is a dog\")\n","  else:\n","    print(fn + \" is a cat\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hUyeUcFofU1u","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}